{"cells":[{"cell_type":"markdown","source":["**ML | Breast Cancer Wisconsin Diagnosis Prediction using Logistic Regression**\n","\n","Breast Cancer Wisconsin Diagnosis dataset is commonly used in machine learning to classify breast tumors as malignant (cancerous) or benign (non-cancerous) based on features extracted from breast mass images. In this project I will apply Logistic Regression algorithm for binary classification to predict the nature of breast tumors. It is ideal for such tasks where the goal is to classify data into two categories. I will walk through the steps of preprocessing the data, training the model and evaluating its performance, demonstrating how this model work in early detection and diagnosis of breast cancer."],"metadata":{"id":"v8SuTn9M8Yvx"}},{"cell_type":"code","source":["# 1. Loading Libraries\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split"],"metadata":{"id":"L8GT4mE3w2FC","executionInfo":{"status":"ok","timestamp":1758216078919,"user_tz":-360,"elapsed":1721,"user":{"displayName":"TASIKUL ISLAM 2.4231E+14","userId":"14086956503266202497"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["# 2. Loading dataset\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r_ZLaUZ72LPI","executionInfo":{"status":"ok","timestamp":1758216363844,"user_tz":-360,"elapsed":3417,"user":{"displayName":"TASIKUL ISLAM 2.4231E+14","userId":"14086956503266202497"}},"outputId":"d02eb308-bb3e-471d-acc6-221b4770432e"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["data = pd.read_csv(\"/content/drive/MyDrive/Machine_Learning/Project-2/data.csv\")\n","print (data.head)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"h-xDeA2G3N5V","executionInfo":{"status":"ok","timestamp":1758216425535,"user_tz":-360,"elapsed":571,"user":{"displayName":"TASIKUL ISLAM 2.4231E+14","userId":"14086956503266202497"}},"outputId":"52af92de-b198-4164-ae48-7db2e283d374"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["<bound method NDFrame.head of            id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n","0      842302         M        17.99         10.38          122.80     1001.0   \n","1      842517         M        20.57         17.77          132.90     1326.0   \n","2    84300903         M        19.69         21.25          130.00     1203.0   \n","3    84348301         M        11.42         20.38           77.58      386.1   \n","4    84358402         M        20.29         14.34          135.10     1297.0   \n","..        ...       ...          ...           ...             ...        ...   \n","564    926424         M        21.56         22.39          142.00     1479.0   \n","565    926682         M        20.13         28.25          131.20     1261.0   \n","566    926954         M        16.60         28.08          108.30      858.1   \n","567    927241         M        20.60         29.33          140.10     1265.0   \n","568     92751         B         7.76         24.54           47.92      181.0   \n","\n","     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n","0            0.11840           0.27760         0.30010              0.14710   \n","1            0.08474           0.07864         0.08690              0.07017   \n","2            0.10960           0.15990         0.19740              0.12790   \n","3            0.14250           0.28390         0.24140              0.10520   \n","4            0.10030           0.13280         0.19800              0.10430   \n","..               ...               ...             ...                  ...   \n","564          0.11100           0.11590         0.24390              0.13890   \n","565          0.09780           0.10340         0.14400              0.09791   \n","566          0.08455           0.10230         0.09251              0.05302   \n","567          0.11780           0.27700         0.35140              0.15200   \n","568          0.05263           0.04362         0.00000              0.00000   \n","\n","     ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n","0    ...          17.33           184.60      2019.0           0.16220   \n","1    ...          23.41           158.80      1956.0           0.12380   \n","2    ...          25.53           152.50      1709.0           0.14440   \n","3    ...          26.50            98.87       567.7           0.20980   \n","4    ...          16.67           152.20      1575.0           0.13740   \n","..   ...            ...              ...         ...               ...   \n","564  ...          26.40           166.10      2027.0           0.14100   \n","565  ...          38.25           155.00      1731.0           0.11660   \n","566  ...          34.12           126.70      1124.0           0.11390   \n","567  ...          39.42           184.60      1821.0           0.16500   \n","568  ...          30.37            59.16       268.6           0.08996   \n","\n","     compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n","0              0.66560           0.7119                0.2654          0.4601   \n","1              0.18660           0.2416                0.1860          0.2750   \n","2              0.42450           0.4504                0.2430          0.3613   \n","3              0.86630           0.6869                0.2575          0.6638   \n","4              0.20500           0.4000                0.1625          0.2364   \n","..                 ...              ...                   ...             ...   \n","564            0.21130           0.4107                0.2216          0.2060   \n","565            0.19220           0.3215                0.1628          0.2572   \n","566            0.30940           0.3403                0.1418          0.2218   \n","567            0.86810           0.9387                0.2650          0.4087   \n","568            0.06444           0.0000                0.0000          0.2871   \n","\n","     fractal_dimension_worst  Unnamed: 32  \n","0                    0.11890          NaN  \n","1                    0.08902          NaN  \n","2                    0.08758          NaN  \n","3                    0.17300          NaN  \n","4                    0.07678          NaN  \n","..                       ...          ...  \n","564                  0.07115          NaN  \n","565                  0.06637          NaN  \n","566                  0.07820          NaN  \n","567                  0.12400          NaN  \n","568                  0.07039          NaN  \n","\n","[569 rows x 33 columns]>\n"]}]},{"cell_type":"code","source":["#Getting Information about the dataset.\n","data.info()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"GC28SocA3dmh","executionInfo":{"status":"ok","timestamp":1758216472991,"user_tz":-360,"elapsed":37,"user":{"displayName":"TASIKUL ISLAM 2.4231E+14","userId":"14086956503266202497"}},"outputId":"08aa928c-4343-4e55-c97f-ae56b9d0d4fb"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 569 entries, 0 to 568\n","Data columns (total 33 columns):\n"," #   Column                   Non-Null Count  Dtype  \n","---  ------                   --------------  -----  \n"," 0   id                       569 non-null    int64  \n"," 1   diagnosis                569 non-null    object \n"," 2   radius_mean              569 non-null    float64\n"," 3   texture_mean             569 non-null    float64\n"," 4   perimeter_mean           569 non-null    float64\n"," 5   area_mean                569 non-null    float64\n"," 6   smoothness_mean          569 non-null    float64\n"," 7   compactness_mean         569 non-null    float64\n"," 8   concavity_mean           569 non-null    float64\n"," 9   concave points_mean      569 non-null    float64\n"," 10  symmetry_mean            569 non-null    float64\n"," 11  fractal_dimension_mean   569 non-null    float64\n"," 12  radius_se                569 non-null    float64\n"," 13  texture_se               569 non-null    float64\n"," 14  perimeter_se             569 non-null    float64\n"," 15  area_se                  569 non-null    float64\n"," 16  smoothness_se            569 non-null    float64\n"," 17  compactness_se           569 non-null    float64\n"," 18  concavity_se             569 non-null    float64\n"," 19  concave points_se        569 non-null    float64\n"," 20  symmetry_se              569 non-null    float64\n"," 21  fractal_dimension_se     569 non-null    float64\n"," 22  radius_worst             569 non-null    float64\n"," 23  texture_worst            569 non-null    float64\n"," 24  perimeter_worst          569 non-null    float64\n"," 25  area_worst               569 non-null    float64\n"," 26  smoothness_worst         569 non-null    float64\n"," 27  compactness_worst        569 non-null    float64\n"," 28  concavity_worst          569 non-null    float64\n"," 29  concave points_worst     569 non-null    float64\n"," 30  symmetry_worst           569 non-null    float64\n"," 31  fractal_dimension_worst  569 non-null    float64\n"," 32  Unnamed: 32              0 non-null      float64\n","dtypes: float64(31), int64(1), object(1)\n","memory usage: 146.8+ KB\n"]}]},{"cell_type":"markdown","source":["**3. Processing Dataset**\n","\n","Dropping columns - 'id' and 'Unnamed: 32' as they have no role in prediction\n","\n","**data['diagnosis'].map():** Converts the diagnosis column, which contains 'M' (Malignant) and 'B' (Benign) into binary values. 'M' is converted to 1 and 'B' to 0 making it suitable for logistic regression."],"metadata":{"id":"sLO2L05e35ri"}},{"cell_type":"code","source":["# Preprocess data\n","data.drop(['Unnamed: 32', 'id'], axis=1, inplace=True)\n","data['diagnosis'] = data['diagnosis'].map({'M': 1, 'B': 0})\n","\n","y = data['diagnosis'].values\n","x_data = data.drop(['diagnosis'], axis=1)"],"metadata":{"id":"S387_SeK3l2_","executionInfo":{"status":"ok","timestamp":1758216645164,"user_tz":-360,"elapsed":8,"user":{"displayName":"TASIKUL ISLAM 2.4231E+14","userId":"14086956503266202497"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["**Normalization**\n","\n","Here we will normalize dataset.\n","\n","**x_data - x_data.min():** subtracts the minimum value from each value in the dataset shifting the data so that the smallest value becomes 0.\n","\n","**x_data.max() - x_data.min():** calculates the range of the data (difference between the maximum and minimum values)."],"metadata":{"id":"RDaOVEoJ4WpK"}},{"cell_type":"code","source":["# Normalize features\n","x = (x_data - x_data.min()) / (x_data.max() - x_data.min())"],"metadata":{"id":"8i1aLG5q4WMA","executionInfo":{"status":"ok","timestamp":1758216725170,"user_tz":-360,"elapsed":16,"user":{"displayName":"TASIKUL ISLAM 2.4231E+14","userId":"14086956503266202497"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["**4. Splitting data for training and testing.**\n","\n","**train_test_split: **This function splits your data into two parts one for training your model and another for testing.\n","\n","**test_size=0.15:** 15% of the data will be used for testing and 85% for training.\n","\n","**x_train = x_train.T:** Transpose (T) to ensure that the data has the correct shape for matrix operations during the logistic regression."],"metadata":{"id":"OeZKOyBK4l9A"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","x_train, x_test, y_train, y_test = train_test_split(\n","    x, y, test_size = 0.15, random_state = 42)\n","\n","x_train = x_train.T\n","x_test = x_test.T\n","y_train = y_train.T\n","y_test = y_test.T\n","\n","print(\"x train: \", x_train.shape)\n","print(\"x test: \", x_test.shape)\n","print(\"y train: \", y_train.shape)\n","print(\"y test: \", y_test.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hKZ9aaTl4zZF","executionInfo":{"status":"ok","timestamp":1758217201965,"user_tz":-360,"elapsed":28,"user":{"displayName":"TASIKUL ISLAM 2.4231E+14","userId":"14086956503266202497"}},"outputId":"08caf3f2-a9e5-4d33-e6a1-39613e697c00"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["x train:  (30, 483)\n","x test:  (30, 86)\n","y train:  (483,)\n","y test:  (86,)\n"]}]},{"cell_type":"markdown","source":["**5. Initializing Model Architecture**"],"metadata":{"id":"47YKOvPV6dBI"}},{"cell_type":"code","source":["# Initializing Weight and bias\n","def initialize_weights_and_bias(dimension):\n","    w = np.random.randn(dimension, 1) * 0.01  # Initialize with small random values\n","    b = 0.0\n","    return w, b"],"metadata":{"id":"VuO2eiYd6eth","executionInfo":{"status":"ok","timestamp":1758217275678,"user_tz":-360,"elapsed":46,"user":{"displayName":"TASIKUL ISLAM 2.4231E+14","userId":"14086956503266202497"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["**Sigmoid Function to calculate z value.**\n","\n","**sigmoid():** It squashes the input value z between 0 and 1 making it suitable for binary classification."],"metadata":{"id":"NmUpCss76o_3"}},{"cell_type":"code","source":["def sigmoid(z):\n","    return 1 / (1 + np.exp(-z))"],"metadata":{"id":"liPTGcmA6sam","executionInfo":{"status":"ok","timestamp":1758217307436,"user_tz":-360,"elapsed":32,"user":{"displayName":"TASIKUL ISLAM 2.4231E+14","userId":"14086956503266202497"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["**Forward-Backward Propagation**\n","\n","**np.dot(w.T, x_train):** Computes the matrix multiplication of the weights and the input data.\n","\n","**cost = (-1/m) * np.sum(y_train * np.log(y_head) + (1 - y_train) * np.log(1 - y_head)):** Measures the difference between the predicted probability (y_head) and true label (y_train).\n","\n","**derivative_weight = (1/m) * np.dot(x_train, (y_head - y_train).T):** This calculates the gradient of the cost with respect to the weights w. It tells us how much we need to change the weights to reduce the cost.\n","\n","**derivative_bias = (1/m) * np.sum(y_head - y_train):** This computes the gradient of the cost with respect to the bias b. It is simply the average of the difference between predicted probabilities (y_head) and actual labels (y_train)."],"metadata":{"id":"5vXgyRzA6zhJ"}},{"cell_type":"code","source":["def forward_backward_propagation(w, b, x_train, y_train):\n","    m = x_train.shape[1]\n","    z = np.dot(w.T, x_train) + b\n","    y_head = sigmoid(z)\n","\n","\n","    cost = (-1/m) * np.sum(y_train * np.log(y_head) + (1 - y_train) * np.log(1 - y_head))\n","\n","    derivative_weight = (1/m) * np.dot(x_train, (y_head - y_train).T)\n","    derivative_bias = (1/m) * np.sum(y_head - y_train)\n","\n","    gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias}\n","    return cost, gradients"],"metadata":{"id":"QiI1o5xA6ym5","executionInfo":{"status":"ok","timestamp":1758217447467,"user_tz":-360,"elapsed":24,"user":{"displayName":"TASIKUL ISLAM 2.4231E+14","userId":"14086956503266202497"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["**Updating Parameters**\n","\n","**w -= learning_rate * gradients[\"derivative_weight\"] and b -= learning_rate * gradients[\"derivative_bias\"]:** Weight(w) and Bias(b) are updated by subtracting the gradient scaled by the learning rate."],"metadata":{"id":"Pf3o0D-47TZv"}},{"cell_type":"code","source":["def update(w, b, x_train, y_train, learning_rate, num_iterations):\n","    costs = []\n","    gradients = {}\n","    for i in range(num_iterations):\n","        cost, grad = forward_backward_propagation(w, b, x_train, y_train)\n","        w -= learning_rate * grad[\"derivative_weight\"]\n","        b -= learning_rate * grad[\"derivative_bias\"]\n","\n","        if i % 100 == 0:\n","            costs.append(cost)\n","            print(f\"Cost after iteration {i}: {cost}\")\n","\n","    parameters = {\"weight\": w, \"bias\": b}\n","    return parameters, gradients, costs"],"metadata":{"id":"-1Q0ky8r7gcv","executionInfo":{"status":"ok","timestamp":1758217527146,"user_tz":-360,"elapsed":44,"user":{"displayName":"TASIKUL ISLAM 2.4231E+14","userId":"14086956503266202497"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["**6. Making Predictions**\n","\n","***np.dot(w.T, x_test):*** This performs a matrix multiplication between the transposed weights (w.T) and test data (x_test).\n","\n","***sigmoid(z):*** Applies the sigmoid activation function to the logits, this function maps the values to the range [0, 1].\n","\n","***z[0, i] > 0.5:*** If the probability for the positive class (class 1) is greater than 0.5 then prediction is 1 otherwise it is 0."],"metadata":{"id":"SgM0Rcua7slX"}},{"cell_type":"code","source":["def predict(w, b, x_test):\n","    m = x_test.shape[1]\n","    y_prediction = np.zeros((1, m))\n","    z = sigmoid(np.dot(w.T, x_test) + b)\n","\n","    for i in range(z.shape[1]):\n","        y_prediction[0, i] = 1 if z[0, i] > 0.5 else 0\n","\n","    return y_prediction"],"metadata":{"id":"k6phuiWR75sW","executionInfo":{"status":"ok","timestamp":1758217630127,"user_tz":-360,"elapsed":9,"user":{"displayName":"TASIKUL ISLAM 2.4231E+14","userId":"14086956503266202497"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["**Logistic Regression**\n","\n","**logistic_regression(x_train, y_train, x_test, y_test, learning_rate=0.01, num_iterations=1000):** This line runs the logistic regression model with the given training and test data, a learning rate of 0.01 and 1000 iterations for training."],"metadata":{"id":"FbOkSQ_Q8ARu"}},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7_PZ6lqtX3pB","outputId":"ad365d3c-3ebb-4ea8-9521-ba11c6a1779c","collapsed":true,"executionInfo":{"status":"ok","timestamp":1758217673050,"user_tz":-360,"elapsed":268,"user":{"displayName":"TASIKUL ISLAM 2.4231E+14","userId":"14086956503266202497"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Cost after iteration 0: 0.6923864756917433\n","Cost after iteration 100: 0.6651584286832788\n","Cost after iteration 200: 0.640772226400971\n","Cost after iteration 300: 0.6184354604101723\n","Cost after iteration 400: 0.5978537847391072\n","Cost after iteration 500: 0.5788432059290065\n","Cost after iteration 600: 0.5612519202377051\n","Cost after iteration 700: 0.544944781830411\n","Cost after iteration 800: 0.5297999882891344\n","Cost after iteration 900: 0.5157079094134956\n","Train accuracy: 90.47619047619048%\n","Test accuracy: 88.37209302325581%\n"]}],"source":["def logistic_regression(x_train, y_train, x_test, y_test, learning_rate=0.01, num_iterations=1000):\n","    dimension = x_train.shape[0]\n","    w, b = initialize_weights_and_bias(dimension)\n","    parameters, gradients, costs = update(w, b, x_train, y_train, learning_rate, num_iterations)\n","\n","    y_prediction_test = predict(parameters[\"weight\"], parameters[\"bias\"], x_test)\n","    y_prediction_train = predict(parameters[\"weight\"], parameters[\"bias\"], x_train)\n","\n","    print(f\"Train accuracy: {100 - np.mean(np.abs(y_prediction_train - y_train)) * 100}%\")\n","    print(f\"Test accuracy: {100 - np.mean(np.abs(y_prediction_test - y_test)) * 100}%\")\n","\n","# Run logistic regression\n","logistic_regression(x_train, y_train, x_test, y_test, learning_rate=0.01, num_iterations=1000)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}